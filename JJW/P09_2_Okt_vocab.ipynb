{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okt로 어휘사전 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "Okt = Okt()\n",
    "\n",
    "file3 = pd.read_csv('../DATA/king_3.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "with open('../DATA/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stop_words = f.readlines()\n",
    "stop_words = [x.strip() for x in stop_words]\n",
    "# print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab = {}\n",
    "data = file3['text'].values\n",
    "sentences = []\n",
    "for line in data:\n",
    "    # 토크나이징\n",
    "    tokens = Okt.nouns(line)\n",
    "    # 정규표현식으로 한글만 추출\n",
    "    tokens = [token for token in tokens if re.match(r'^[가-힣]+$', token)]\n",
    "    \n",
    "    # 불용어 제거\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # 빈도수 계산\n",
    "    for token in tokens:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = 1\n",
    "        else:\n",
    "            vocab[token] += 1\n",
    "    # 데이터프레임에 추가\n",
    "    sentences.append(tokens)\n",
    "\n",
    "vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 29476\n",
      "[말: 24436], [사람: 17938], [하니: 16808], [임금: 12539], [수: 10977], [명: 10277], [은: 9165], [청: 7035], [지금: 6769], [신: 6623], [그대로: 6610], [죄: 6576], [도: 6503], [여러: 6314], [관: 6242], [고: 5944], [백성: 5728], [전하: 5670], [내: 5670], [이제: 5473], [전: 5465], [생각: 5389], [알: 5386], [법: 5291], [의논: 5123], [위: 5086], [나라: 5009], [백: 4729], [행: 4720], [다시: 4705], [두: 4633], [뒤: 4606], [하소: 4449], [번: 4441], [정: 4296], [곳: 4286], [계: 4245], [중: 4189], [가지: 4177], [함: 4152], [마음: 4019], [못: 3941], [인도: 3897], [사신: 3870], [뜻: 3821], [예조: 3784], [인: 3776], [하라: 3763], [자가: 3761], [데: 3700], [제사: 3617], [자리: 3595], [보고: 3462], [앞: 3449], [필: 3390], [설치: 3370], [때문: 3362], [품: 3356], [군사: 3292], [고을: 3205], [상: 3107], [감사: 3040], [세: 3036], [병조: 2990], [안: 2917], [전지: 2877], [좌: 2865], [의정부: 2819], [서로: 2760], [거: 2757], [종: 2726], [집: 2669], [정사: 2623], [바: 2585], [수령: 2570], [찬: 2508], [몸: 2481], [후: 2420], [술: 2396], [온: 2341], [이하: 2258], [부: 2252], [의거: 2235], [향: 2219], [장: 2203], [판서: 2199], [시행: 2186], [비: 2185], [신하: 2171], [관원: 2158], [석: 2154], [적: 2152], [날: 2144], [집사: 2143], [만: 2124], [형조: 2091], [판: 2082], [일찍이: 2071], [통례: 2054], [경연: 2045], [처: 2031], "
     ]
    }
   ],
   "source": [
    "print('vocab size:', len(vocab))\n",
    "# vocab 일부만 출력\n",
    "i = 0\n",
    "for word, freq in vocab:\n",
    "    print(f'[{word}: {freq}]', end=', ')\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab.txt 저장\n",
    "with open('../DATA/vocab/vocab3.txt', 'w', encoding='utf-8') as f:\n",
    "    for word, freq in vocab:\n",
    "        f.write(f'{word}: {freq}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     vocabs \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# dict로 변환\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     vocabs \u001b[38;5;241m=\u001b[39m {x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]: \u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vocab}\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(vocabs[:\u001b[38;5;241m10\u001b[39m])\n",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m     vocabs \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# dict로 변환\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     vocabs \u001b[38;5;241m=\u001b[39m {\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]: \u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vocab}\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(vocabs[:\u001b[38;5;241m10\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# vocab 불러오기\n",
    "with open('../DATA/vocab/vocab3.txt', 'r', encoding='utf-8') as f:\n",
    "    vocabs = f.readlines()\n",
    "    # dict로 변환\n",
    "    vocabs = {x.split(':')[0]: int(x.split(':')[1]) for x in vocab}\n",
    "    print(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# vocab에서 글자수가 1개인 단어 제거\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m vocab_no1 \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vocab \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(vocab_no1))\n",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# vocab에서 글자수가 1개인 단어 제거\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m vocab_no1 \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vocab \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(vocab_no1))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# vocab에서 글자수가 1개인 단어 제거\n",
    "vocab_no1 = [x for x in vocab if len(x.split(':')[0]) > 1]\n",
    "print(len(vocab_no1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 : 워드클라우드\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wc = WordCloud(font_path='C:/Windows/Fonts/malgun.ttf', background_color='white', width=800, height=600, colormap='tab20b')\n",
    "cloud = wc.generate_from_frequencies(dict(vocab))\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.axis('off')\n",
    "plt.imshow(cloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3['tokenized'] = sentences\n",
    "file3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3.to_csv('../DATA/okt/king_3_tokenized.csv', encoding='utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임베딩 함수 사용 : EmbeddingBag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install -c pytorch torchtext\n",
    "# %conda update -n base -c defaults conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 지신사(知申事) 정흠지(鄭欽之)가 포천(抱川)에 가서 그의 어머니를 뵙고 오기를 청하니, 이를 허락하고 인하여 술과 고기를 내려 주었다.\n",
      "['신사', '정흠', '포천', '그', '어머니', '오기', '청', '허락', '술', '고기']\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "for line in file3['text'].values:\n",
    "    print(line)\n",
    "    print(Okt.nouns(line))\n",
    "    # iter() : 반복자 객체 생성\n",
    "    print('---------------------------------')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '것', '이', '의', '그', '말', '를', '사람', '하니', '등']\n"
     ]
    }
   ],
   "source": [
    "# 이터레이터 토큰화 함수\n",
    "from konlpy.tag import Okt\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "Okt = Okt()\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        # print(text)\n",
    "        yield iter(Okt.nouns(text))\n",
    "        \n",
    "# next(yield_tokens(file3['text'])) # 확인함\n",
    "\n",
    "# 파일 읽기\n",
    "file3 = pd.read_csv('../DATA/king_3.csv', encoding='utf-8', sep=';')\n",
    "data = file3['text'].values\n",
    "\n",
    "# vocab 생성\n",
    "vocab = build_vocab_from_iterator(yield_tokens(data), specials=[\"<unk>\"])\n",
    "# vocab 확인\n",
    "print(vocab.get_itos()[:10])\n",
    "\n",
    "# <unk> 인덱스를 0으로 설정\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29467\n",
      "['<unk>', '말', '사람', '임금', '명', '청', '신', '죄', '백성', '전하', '이제', '전', '생각', '알', '법', '의논', '위', '나라', '백', '행']\n",
      "28897\n",
      "['<unk>', '사람', '하니', '임금', '모두', '지금', '그대로', '여러', '비록', '백성', '전하', '어찌', '이제', '생각', '의논', '나라', '또한', '우리', '다시', '하소']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "with open('../DATA/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stop_words = f.readlines()\n",
    "stop_words = [x.strip() for x in stop_words]\n",
    "stop_words += ['하니', '수', '은', '지금', '그대로', '도', '여러', '관', '고', '내', 이제, 전, 생각]\n",
    "\n",
    "vocab_no_stop = [x for x in vocab.get_itos() if x not in stop_words]\n",
    "print(len(vocab_no_stop))\n",
    "print(vocab_no_stop[:20])\n",
    "\n",
    "# 글자수가 1개인 단어 제거\n",
    "vocab_no1 = [x for x in vocab.get_itos() if len(x) > 1]\n",
    "print(len(vocab_no1))\n",
    "print(vocab_no1[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시\n",
    "vocab['사람'] # 487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Vocab' object has no attribute 'freqs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mlen\u001b[39m(vocab\u001b[38;5;241m.\u001b[39mget_itos())\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 빈도수 확인\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreqs\u001b[49m\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Vocab' object has no attribute 'freqs'"
     ]
    }
   ],
   "source": [
    "# vocab 확인\n",
    "len(vocab.get_itos())\n",
    "\n",
    "# 빈도수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EmbeddingBag을 포함한 RNN 모델\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # 데이터셋 클래스\n",
    "# class Dataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, data, labels):\n",
    "#         # data = file3['tokenized'].values; labels = file3['label'].values\n",
    "#         self.data = \n",
    "#         self.labels = labels\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
